library(likert)
library(psych)
library(ordinal)
hebrank$Q1 = factor(webrank$dukame,
levels = c("0", "1", "2", "3", "4", "5"),
ordered = TRUE)
library(dunn.test)
df_merge<-rbind(df_eng,df_heb)
webrank=df_merge[c(1,12, 14, 17, 19,20)]
webrank$part_ID<-1:100
library(likert)
library(psych)
library(ordinal)
webrank$dukame = factor(webrank$dukame,
levels = c("0", "1", "2", "3", "4", "5"),
ordered = TRUE)
webrank$gedino = factor(webrank$gedino,
levels = c("0", "1", "2", "3", "4", "5"),
ordered = TRUE)
webrank$kimuga = factor(webrank$kimuga,
levels = c("0", "1", "2", "3", "4", "5"),
ordered = TRUE)
webrank$nalobi = factor(webrank$nalobi,
levels = c("0", "1", "2", "3", "4", "5"),
ordered = TRUE)
webrank$tobelu= factor(webrank$tobelu,
levels = c("0", "1", "2", "3", "4", "5"),
ordered = TRUE)
headTail(webrank)
summary(webrank)
results=likert(webrank)
library(dunn.test)
df_merge<-rbind(df_eng,df_heb)
webrank=df_merge[c(1,12, 14, 17, 19,20)]
webrank$part_ID<-1:100
webrank=melt(webrank,id=1)
colnames(webrank)[2]="item"
colnames(webrank)[3]="ranking"
kruskal.test(ranking ~ item,
data = webrank)
DT = dunn.test(webrank$ranking,webrank$item, method="bh")      # Adjusts p-values for multiple comparisons;
View(df)
m1 = lm(trial_accuracy~web_ratings,data=df)
summary(m1)
m1 = lm(trial_accuracy~web_ratings,data=df)
summary(m1)
m1 = lm(trial_accuracy~fam_ratings,data=df)
summary(m1)
m1 = lm(trial_accuracy~web_ratings,data=df)
summary(m1)
m1 = lm(trial_accuracy~fam_ratings + web_ratings,data=df)
summary(m1)
View(hebrank)
eng_ling_trial=subset(eng_trial,task=="1")
eng_ling_trial$fam_ratings=as.factor(as.character(eng_ling_trial$fam_ratings))
contrasts(eng_ling_trial$fam_ratings)=centerfactor(eng_ling_trial$fam_ratings)
m1 = glmer(trial_accuracy~fam_ratings+(1|subject),family = binomial,data=eng_ling_trial)
summary(m1)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
#Loading library
library(psych)
library(readr)
library(optimx)
library(corrplot)
library(reshape)
library(reshape2)
library(lmerTest)
library(ggplot2)
library(scales)
library(ggbeeswarm)
library(Hmisc)
library(arm)
library(ez)
library(dplyr)
source("centerfactor.R")
#Load data
hebtrial <- read.csv('../data/heb_trial_clean_082819.csv')
hebindiv <- read.csv('../data/HEB_Data.csv')
hebtrial$task = as.factor(as.character(hebtrial$task)) # 0 means nonlinguistic and 1 means linguistic task
hebtrial$task_order = as.factor(as.character(hebtrial$task_order)) # 0 means nonlinguistic task comes first and 1 means linguistic task comes first
hebtrial$trial_order = as.factor(as.character(hebtrial$trial_order)) # 0 means foil comes first and 1 means target comes first
hebtrial$subject = as.factor(as.character(hebtrial$subject))
hebtrial$trial = as.factor(as.character(hebtrial$trial))
eng_trial<- hebtrial[which(hebtrial$language == "english"),]
eng_trial=droplevels(eng_trial)
contrasts(eng_trial$trial)=centerfactor(eng_trial$trial)
summary(eng_trial)
eng.lmer <- glmer(trial_accuracy ~ 1 + task_order + task + trial_order + trial + gender + (1|subject), family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)), data = eng_trial)
summary(eng.lmer)
eng.lmer <- glmer(trial_accuracy ~ 1 + task_order + task + (1|subject), family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)), data = eng_trial)
summary(eng.lmer)
eng_trial$task_code = 0
eng_trial[eng_trial$task=="0",]$task_code = 1
eng.ling.lmer <- glmer(trial_accuracy ~ 1 + task_order + task_code + trial_order + task_code:trial_order + (1+trial_order|subject), family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)), data = eng_trial)
summary(eng.ling.lmer)
eng_trial$task_code = 0
eng_trial[eng_trial$task=="1",]$task_code = 1
eng.nonling.lmer <- glmer(trial_accuracy ~ 1 + task_order + task_code + trial_order + task_code:trial_order + (1+trial_order|subject), family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)), data = eng_trial)
summary(eng.nonling.lmer)
eng.lmer <- glmer(trial_accuracy ~ 1 + task + (1|subject), family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)), data = eng_trial)
summary(eng.lmer)
# remove two participants who only had one task
hebindiv=hebindiv[is.na(hebindiv$accuracy)==0,]
hebindiv=subset(hebindiv,subject!="a_005")
hebindiv=subset(hebindiv,subject!="a_026")
eng_indiv=subset(hebindiv,group=="English")
summary(eng_indiv)
taskanova <- ezANOVA(eng_indiv, accuracy, subject, within = .(task), type = 2, detailed = TRUE)
taskanova
eng_ling_indiv = subset(eng_indiv, Task = "Linguistic")
hist(eng_ling_indiv$familiarity)
m1 = lm(accuracy~familiarity,data=eng_indiv[eng_indiv$task=="Linguistic",])
summary(m1)
colnames(eng_indiv)[8]="Task"
ggplot() +
theme_classic(base_size = 20.0) +
xlab(label = 'Vocabuary (PVT)') +
ylab(label = 'Overall Accuracy') +
scale_x_continuous(breaks = pretty_breaks()) +
scale_y_continuous(breaks = pretty_breaks()) +
geom_point(aes(x = vocabulary,y = accuracy,colour = Task),data=eng_indiv, shape = 23, size = 3) +
geom_smooth(aes(x = vocabulary,y = accuracy,colour = Task),data=eng_indiv,method = lm,formula = 'y ~ x',se = FALSE)
View(eng_ling_indiv)
View(eng_trial)
m1 = lm(trial_accuracy~fam_ratings,data=eng_trial[eng_trial$task=="Linguistic",])
View(eng.ling.lmer)
data=eng_trial[eng_trial$task=="Linguistic",]
eng_trial_ling = subset(eng_trial, Task = "Linguistic")
m1 = lm(trial_accuracy~fam_ratings,data=eng_trial_ling)
summary(m1)
eng_ling_trial=subset(eng_trial,task=="1")
eng_ling_trial$fam_ratings=as.factor(as.character(eng_ling_trial$fam_ratings))
contrasts(eng_ling_trial$fam_ratings)=centerfactor(eng_ling_trial$fam_ratings)
m1 = glmer(trial_accuracy~fam_ratings+(1|subject),family = binomial,data=eng_ling_trial)
summary(m1)
glmvoc = glm(accuracy~vocabulary*Task,data=eng_indiv)
summary(glmvoc)
colnames(eng_indiv)[8]="Task"
ggplot() +
theme_classic(base_size = 20.0) +
xlab(label = 'Vocabuary (PVT)') +
ylab(label = 'Overall Accuracy') +
scale_x_continuous(breaks = pretty_breaks()) +
scale_y_continuous(breaks = pretty_breaks()) +
geom_point(aes(x = vocabulary,y = accuracy,colour = Task),data=eng_indiv, shape = 23, size = 3) +
geom_smooth(aes(x = vocabulary,y = accuracy,colour = Task),data=eng_indiv,method = lm,formula = 'y ~ x',se = FALSE)
knitr::opts_chunk$set(echo = TRUE)
#Loading library
library(psych)
library(readr)
library(optimx)
library(corrplot)
library(reshape)
library(reshape2)
library(lmerTest)
library(ggplot2)
library(scales)
library(ggbeeswarm)
library(Hmisc)
library(arm)
library(ez)
library(dplyr)
library(tidyr)
library(simr)
library(dunn.test)
library(likert)
library(psych)
library(ordinal)
#library(EMAtools)
library(lme4)
source("centerfactor.R")
hebtrial <- read.csv('/Volumes/GoogleDrive/My Drive/Projects/HEB/R_scripts/heb_trial_clean_082819.csv')
hebtrial$task = as.factor(as.character(hebtrial$task)) # 0 means nonlinguistic and 1 means linguistic task
hebtrial$task_order = as.factor(as.character(hebtrial$task_order)) # 0 means nonlinguistic task comes first and 1 means linguistic task comes first
hebtrial$trial_order = as.factor(as.character(hebtrial$trial_order)) # 0 means foil comes first and 1 means target comes first
hebtrial$subject = as.factor(as.character(hebtrial$subject))
hebtrial$trial = as.factor(as.character(hebtrial$trial))
heb_only_trial<- hebtrial[which(hebtrial$language == "hebrew"),]
eng_trial<- hebtrial[which(hebtrial$language == "english"),]
eng_ling_trial<- eng_trial[which(eng_trial$task == 1),]
eng_trial=droplevels(eng_trial)
contrasts(eng_trial$trial)=centerfactor(eng_trial$trial)
summary(eng_trial)
hebindiv <- read.csv('/Volumes/GoogleDrive/My Drive/Projects/HEB/R_scripts/HEB_Data.csv')
hebindiv=hebindiv[is.na(hebindiv$accuracy)==0,]
hebindiv=subset(hebindiv,subject!="a_005")
hebindiv=subset(hebindiv,subject!="a_026")
hebindiv=subset(hebindiv,subject!="a_035")
hebindiv=subset(hebindiv,subject!="a_050")
eng_indiv=subset(hebindiv,group=="English")
heb_only_indiv=subset(hebindiv,group=="Hebrew")
hebindiv_ling_eng<- eng_indiv[which(eng_indiv$Task == "Linguistic"),]
#hebindiv_eng<-hebindiv[which(hebindiv$group=="English"),]
hebindiv_ling_eng<- eng_indiv[which(eng_indiv$Task == "Linguistic"),]
hebindiv_nonling_eng<- eng_indiv[which(eng_indiv$Task == "Non-Linguistic"),]
t.test(hebindiv_ling_eng$accuracy, mu=50, alternative = "greater")
sd(hebindiv_ling_eng$accuracy)
t.test(hebindiv_nonling_eng$accuracy, mu=50, alternative = "greater")
sd(hebindiv_nonling_eng$accuracy)
hebindiv_ling_eng_accurate <- hebindiv_ling_eng[which(hebindiv_ling_eng$accuracy > 50),]
length(unique(hebindiv_ling_eng_accurate$subject))
hebindiv_nonling_eng_accurate <- hebindiv_nonling_eng[which(hebindiv_nonling_eng$accuracy > 50),]
length(unique(hebindiv_nonling_eng_accurate$subject))
eng_trial<-eng_trial[-c(1:3)]
fm1 <- lmer(trial_accuracy ~ 1 + task + (1|subject), REML = F, data = eng_trial)
fm2 <- lmer(trial_accuracy ~ 1 + task + task_order + (1|subject), REML = F, data = eng_trial)
AIC(fm1, fm2)
BIC(fm1, fm2)
dev0 <- -2*logLik(fm1) # deviance simpler model
dev1 <- -2*logLik(fm2) # deviance complex model
devdiff <- as.numeric(dev0-dev1) # difference in deviances
dfdiff <- attr(dev1,"df")-attr(dev0,"df") # difference in params (using dfs)
cat('Chi-square =', devdiff, '(df=', dfdiff,'), p =',
pchisq(devdiff,dfdiff,lower.tail=FALSE))
summary(fm1)
#power<-powerSim(fm1, fixed("task"), nsim = 500)
#power
#lme.dscore(fm1, eng_trial, "lme4")
# remove NA's
hebrank=subset(hebindiv_ling_eng)
hebrank=hebrank[c(11:15)]
hebrank$Q1 = factor(hebrank$Q1,
levels = c("0", "1", "2", "3", "4", "5"),
ordered = TRUE)
hebrank$Q2 = factor(hebrank$Q2,
levels = c("0", "1", "2", "3", "4", "5"),
ordered = TRUE)
hebrank$Q3 = factor(hebrank$Q3,
levels = c("0", "1", "2", "3", "4", "5"),
ordered = TRUE)
hebrank$Q4 = factor(hebrank$Q4,
levels = c("0", "1", "2", "3", "4", "5"),
ordered = TRUE)
hebrank$Q5 = factor(hebrank$Q5,
levels = c("0", "1", "2", "3", "4", "5"),
ordered = TRUE)
headTail(hebrank)
summary(hebrank)
results=likert(hebrank)
summary(results)
plot(results,
type="heat",
low.color = "white",
high.color = "red",
text.color = "black",
text.size = 4,
wrap = 50)
hebrank=eng_indiv[c(1,11:15)]
hebrank=melt(hebrank,id=1)
colnames(hebrank)[2]="item"
colnames(hebrank)[3]="ranking"
kruskal.test(ranking ~ item,
data = hebrank)
DT = dunn.test(hebrank$ranking,hebrank$item, method="bh")
eng_ling_trial$fam_ratings<-as.numeric(eng_ling_trial$fam_ratings)
cor.test(eng_ling_trial$trial_accuracy,eng_ling_trial$fam_ratings, method = "spearman")
cor.test(hebindiv_ling_eng$accuracy,hebindiv_ling_eng$Q1, method = "spearman")
cor.test(hebindiv_ling_eng$accuracy,hebindiv_ling_eng$Q2, method = "spearman")
cor.test(hebindiv_ling_eng$accuracy,hebindiv_ling_eng$Q3, method = "spearman")
cor.test(hebindiv_ling_eng$accuracy,hebindiv_ling_eng$Q4, method = "spearman")
cor.test(hebindiv_ling_eng$accuracy,hebindiv_ling_eng$Q5, method = "spearman")
eng_ling_trial$fam_ratings=as.factor(as.character(eng_ling_trial$fam_ratings))
contrasts(eng_ling_trial$fam_ratings)=centerfactor(eng_ling_trial$fam_ratings)
fm1 <-  glmer(trial_accuracy~fam_ratings+(1|subject),family = binomial,data=eng_ling_trial)
fm2 <-  glmer(trial_accuracy~fam_ratings+(1|subject)+(1|trial_number),family = binomial,data=eng_ling_trial)
AIC(fm1, fm2)
BIC(fm1, fm2)
dev0 <- -2*logLik(fm1) # deviance simpler model
dev1 <- -2*logLik(fm2) # deviance complex model
devdiff <- as.numeric(dev0-dev1) # difference in deviances
dfdiff <- attr(dev1,"df")-attr(dev0,"df") # difference in params (using dfs)
cat('Chi-square =', devdiff, '(df=', dfdiff,'), p =',
pchisq(devdiff,dfdiff,lower.tail=FALSE))
summary(fm2)
#lme.dscore(fm2, eng_ling_trial, "lme4")
cor.test(hebindiv_ling_eng$accuracy,hebindiv_ling_eng$vocabulary, method = "pearson")
cor.test(hebindiv_nonling_eng$accuracy,hebindiv_ling_eng$vocabulary, method = "pearson")
colnames(eng_indiv)[8]="Task"
ggplot() +
theme_classic(base_size = 20.0) +
xlab(label = 'Vocabulary (PVT)') +
ylab(label = 'Overall Accuracy') +
scale_x_continuous(breaks = pretty_breaks()) +
scale_y_continuous(breaks = pretty_breaks()) +
geom_point(aes(x = vocabulary,y = accuracy,colour = Task),data=eng_indiv, shape = 23, size = 3) +
geom_smooth(aes(x = vocabulary,y = accuracy,colour = Task),data=eng_indiv,method = lm,formula = 'y ~ x',se = FALSE)
#hebindiv_eng<-hebindiv[which(hebindiv$group=="English"),]
hebindiv_ling_heb<- heb_only_indiv[which(heb_only_indiv$Task == "Linguistic"),]
hebindiv_nonling_heb<- heb_only_indiv[which(heb_only_indiv$Task == "Non-Linguistic"),]
t.test(hebindiv_ling_heb$accuracy, mu=50, alternative = "greater")
sd(hebindiv_ling_heb$accuracy)
t.test(hebindiv_nonling_heb$accuracy, mu=50, alternative = "greater")
sd(hebindiv_nonling_heb$accuracy)
hebindiv_ling_heb_accurate <- hebindiv_ling_heb[which(hebindiv_ling_heb$accuracy > 50),]
length(unique(hebindiv_ling_heb_accurate$subject))
hebindiv_nonling_heb_accurate <- hebindiv_nonling_heb[which(hebindiv_nonling_heb$accuracy > 50),]
length(unique(hebindiv_nonling_heb_accurate$subject))
hebgroup.lmer <- glmer(trial_accuracy ~ 1 + language * task + (1|subject), family = binomial,control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)), data = hebtrial)
summary(hebgroup.lmer)
#power<-powerSim(hebgroup.lmer,fixed("language*task"), nsim = 200)
#power
#lme.dscore(hebgroup.lmer, hebtrial, "lme4")
hebindiv_eng<-hebindiv[which(hebindiv$group=="English"),]
hebindiv_ling_eng<- hebindiv_eng[which(hebindiv_eng$Task == "Linguistic"),]
hebindiv_nonling_eng<- hebindiv_eng[which(hebindiv_eng$Task == "Non-Linguistic"),]
cor.test(hebindiv_ling_eng$accuracy,hebindiv_nonling_eng$accuracy, method = "pearson")
hebindiv_heb<-hebindiv[which(hebindiv$group=="Hebrew"),]
hebindiv_ling_heb<- hebindiv_heb[which(hebindiv_heb$Task == "Linguistic"),]
hebindiv_nonling_heb<- hebindiv_heb[which(hebindiv_heb$Task == "Non-Linguistic"),]
cor.test(hebindiv_ling_heb$accuracy,hebindiv_nonling_heb$accuracy, method = "pearson")
myvars<- c("subject","language","task","trial", "trial_accuracy")
trial_cronbach <- hebtrial[myvars]
trial_cronbach_eng<-trial_cronbach[which(trial_cronbach$language=="english"),]
trial_cronbach<-trial_cronbach_eng[which(trial_cronbach_eng$task== 1),]
trial_cronbach_spread <- trial_cronbach %>% spread(trial,trial_accuracy,fill = NA, convert = FALSE)
trial_cronbach_clean <- trial_cronbach_spread[ -c(1,2,3) ]
#psych::alpha(trial_cronbach_clean,check.keys=TRUE)
psych::alpha(trial_cronbach_clean, check.keys = TRUE)$total$std.alpha
myvars<- c("subject","language","task","trial", "trial_accuracy")
trial_cronbach <- hebtrial[myvars]
trial_cronbach_eng<-trial_cronbach[which(trial_cronbach$language=="hebrew"),]
trial_cronbach<-trial_cronbach_eng[which(trial_cronbach_eng$task== 1),]
trial_cronbach_spread <- trial_cronbach %>% spread(trial,trial_accuracy,fill = NA, convert = FALSE)
trial_cronbach_clean <- trial_cronbach_spread[ -c(1,2,3) ]
#psych::alpha(trial_cronbach_clean,check.keys=TRUE)
psych::alpha(trial_cronbach_clean, check.keys = TRUE)$total$std.alpha
d <- hebindiv %>%
select(Task, group, accuracy) %>%  # select relevant variables
mutate(task = factor(Task, labels = c("Linguistic", "Non-linguistic")),
group = factor(group))
head(d)
d %>%
group_by(task, group) %>%
summarise(accuracy_mean = mean(accuracy))
sum_d <- d %>%
group_by(task, group) %>%
summarise(accuracy_mean = mean(accuracy),
se   = sd(accuracy)/sqrt(n()))
sum_d
View(hebindiv_ling_eng)
lm(accuracy~familiarity + vocabulary ,data=hebindiv_ling_eng)
m1 <- lm(accuracy~familiarity + vocabulary ,data=hebindiv_ling_eng)
summary(m1)
hebindiv_ling_eng$accuracy_std<-standardize(hebindiv_ling_eng$accuracy)
install.packages("robustHD")
library(robustHD)
hebindiv_ling_eng$accuracy_std<-standardize(hebindiv_ling_eng$accuracy)
install.packages("standardize")
library(standardize)
hebindiv_ling_eng$accuracy_std<-standardize(hebindiv_ling_eng$accuracy)
knitr::opts_chunk$set(echo = TRUE)
#Loading library
library(psych)
library(readr)
library(optimx)
library(corrplot)
library(reshape)
library(reshape2)
library(lmerTest)
library(ggplot2)
library(scales)
library(ggbeeswarm)
library(Hmisc)
library(arm)
library(ez)
library(dplyr)
library(tidyr)
library(simr)
library(dunn.test)
library(likert)
library(psych)
library(ordinal)
#library(EMAtools)
library(lme4)
source("centerfactor.R")
hebtrial <- read.csv('/Volumes/GoogleDrive/My Drive/Projects/HEB/R_scripts/heb_trial_clean_082819.csv')
hebtrial$task = as.factor(as.character(hebtrial$task)) # 0 means nonlinguistic and 1 means linguistic task
hebtrial$task_order = as.factor(as.character(hebtrial$task_order)) # 0 means nonlinguistic task comes first and 1 means linguistic task comes first
hebtrial$trial_order = as.factor(as.character(hebtrial$trial_order)) # 0 means foil comes first and 1 means target comes first
hebtrial$subject = as.factor(as.character(hebtrial$subject))
hebtrial$trial = as.factor(as.character(hebtrial$trial))
heb_only_trial<- hebtrial[which(hebtrial$language == "hebrew"),]
eng_trial<- hebtrial[which(hebtrial$language == "english"),]
eng_ling_trial<- eng_trial[which(eng_trial$task == 1),]
eng_trial=droplevels(eng_trial)
contrasts(eng_trial$trial)=centerfactor(eng_trial$trial)
summary(eng_trial)
hebindiv <- read.csv('/Volumes/GoogleDrive/My Drive/Projects/HEB/R_scripts/HEB_Data.csv')
hebindiv=hebindiv[is.na(hebindiv$accuracy)==0,]
hebindiv=subset(hebindiv,subject!="a_005")
hebindiv=subset(hebindiv,subject!="a_026")
hebindiv=subset(hebindiv,subject!="a_035")
hebindiv=subset(hebindiv,subject!="a_050")
eng_indiv=subset(hebindiv,group=="English")
heb_only_indiv=subset(hebindiv,group=="Hebrew")
hebindiv_ling_eng<- eng_indiv[which(eng_indiv$Task == "Linguistic"),]
#hebindiv_eng<-hebindiv[which(hebindiv$group=="English"),]
hebindiv_ling_eng<- eng_indiv[which(eng_indiv$Task == "Linguistic"),]
hebindiv_nonling_eng<- eng_indiv[which(eng_indiv$Task == "Non-Linguistic"),]
t.test(hebindiv_ling_eng$accuracy, mu=50, alternative = "greater")
sd(hebindiv_ling_eng$accuracy)
t.test(hebindiv_nonling_eng$accuracy, mu=50, alternative = "greater")
sd(hebindiv_nonling_eng$accuracy)
hebindiv_ling_eng_accurate <- hebindiv_ling_eng[which(hebindiv_ling_eng$accuracy > 50),]
length(unique(hebindiv_ling_eng_accurate$subject))
hebindiv_nonling_eng_accurate <- hebindiv_nonling_eng[which(hebindiv_nonling_eng$accuracy > 50),]
length(unique(hebindiv_nonling_eng_accurate$subject))
eng_trial<-eng_trial[-c(1:3)]
fm1 <- lmer(trial_accuracy ~ 1 + task + (1|subject), REML = F, data = eng_trial)
fm2 <- lmer(trial_accuracy ~ 1 + task + task_order + (1|subject), REML = F, data = eng_trial)
AIC(fm1, fm2)
BIC(fm1, fm2)
dev0 <- -2*logLik(fm1) # deviance simpler model
dev1 <- -2*logLik(fm2) # deviance complex model
devdiff <- as.numeric(dev0-dev1) # difference in deviances
dfdiff <- attr(dev1,"df")-attr(dev0,"df") # difference in params (using dfs)
cat('Chi-square =', devdiff, '(df=', dfdiff,'), p =',
pchisq(devdiff,dfdiff,lower.tail=FALSE))
summary(fm1)
#power<-powerSim(fm1, fixed("task"), nsim = 500)
#power
#lme.dscore(fm1, eng_trial, "lme4")
# remove NA's
hebrank=subset(hebindiv_ling_eng)
hebrank=hebrank[c(11:15)]
hebrank$Q1 = factor(hebrank$Q1,
levels = c("0", "1", "2", "3", "4", "5"),
ordered = TRUE)
hebrank$Q2 = factor(hebrank$Q2,
levels = c("0", "1", "2", "3", "4", "5"),
ordered = TRUE)
hebrank$Q3 = factor(hebrank$Q3,
levels = c("0", "1", "2", "3", "4", "5"),
ordered = TRUE)
hebrank$Q4 = factor(hebrank$Q4,
levels = c("0", "1", "2", "3", "4", "5"),
ordered = TRUE)
hebrank$Q5 = factor(hebrank$Q5,
levels = c("0", "1", "2", "3", "4", "5"),
ordered = TRUE)
headTail(hebrank)
summary(hebrank)
results=likert(hebrank)
summary(results)
plot(results,
type="heat",
low.color = "white",
high.color = "red",
text.color = "black",
text.size = 4,
wrap = 50)
hebrank=eng_indiv[c(1,11:15)]
hebrank=melt(hebrank,id=1)
colnames(hebrank)[2]="item"
colnames(hebrank)[3]="ranking"
kruskal.test(ranking ~ item,
data = hebrank)
DT = dunn.test(hebrank$ranking,hebrank$item, method="bh")
eng_ling_trial$fam_ratings<-as.numeric(eng_ling_trial$fam_ratings)
cor.test(eng_ling_trial$trial_accuracy,eng_ling_trial$fam_ratings, method = "spearman")
cor.test(hebindiv_ling_eng$accuracy,hebindiv_ling_eng$Q1, method = "spearman")
cor.test(hebindiv_ling_eng$accuracy,hebindiv_ling_eng$Q2, method = "spearman")
cor.test(hebindiv_ling_eng$accuracy,hebindiv_ling_eng$Q3, method = "spearman")
cor.test(hebindiv_ling_eng$accuracy,hebindiv_ling_eng$Q4, method = "spearman")
cor.test(hebindiv_ling_eng$accuracy,hebindiv_ling_eng$Q5, method = "spearman")
eng_ling_trial$fam_ratings=as.factor(as.character(eng_ling_trial$fam_ratings))
contrasts(eng_ling_trial$fam_ratings)=centerfactor(eng_ling_trial$fam_ratings)
fm1 <-  glmer(trial_accuracy~fam_ratings+(1|subject),family = binomial,data=eng_ling_trial)
fm2 <-  glmer(trial_accuracy~fam_ratings+(1|subject)+(1|trial_number),family = binomial,data=eng_ling_trial)
AIC(fm1, fm2)
BIC(fm1, fm2)
dev0 <- -2*logLik(fm1) # deviance simpler model
dev1 <- -2*logLik(fm2) # deviance complex model
devdiff <- as.numeric(dev0-dev1) # difference in deviances
dfdiff <- attr(dev1,"df")-attr(dev0,"df") # difference in params (using dfs)
cat('Chi-square =', devdiff, '(df=', dfdiff,'), p =',
pchisq(devdiff,dfdiff,lower.tail=FALSE))
summary(fm2)
#lme.dscore(fm2, eng_ling_trial, "lme4")
cor.test(hebindiv_ling_eng$accuracy,hebindiv_ling_eng$vocabulary, method = "pearson")
cor.test(hebindiv_nonling_eng$accuracy,hebindiv_ling_eng$vocabulary, method = "pearson")
colnames(eng_indiv)[8]="Task"
ggplot() +
theme_classic(base_size = 20.0) +
xlab(label = 'Vocabulary (PVT)') +
ylab(label = 'Overall Accuracy') +
scale_x_continuous(breaks = pretty_breaks()) +
scale_y_continuous(breaks = pretty_breaks()) +
geom_point(aes(x = vocabulary,y = accuracy,colour = Task),data=eng_indiv, shape = 23, size = 3) +
geom_smooth(aes(x = vocabulary,y = accuracy,colour = Task),data=eng_indiv,method = lm,formula = 'y ~ x',se = FALSE)
hebindiv_ling_eng$accuracy_std<-standardize(hebindiv_ling_eng$accuracy)
hebindiv_ling_eng$accuracy_std<-standardize(hebindiv_ling_eng$accuracy, centerFun = mean)
hebindiv_ling_eng$accuracy_std<-scale(hebindiv_ling_eng$accuracy)
hebindiv_ling_eng$vocabulary_std<-standardize(hebindiv_ling_eng$vocabulary)
hebindiv_ling_eng$accuracy_std<-scale(hebindiv_ling_eng$accuracy)
hebindiv_ling_eng$vocabulary_std<-scale(hebindiv_ling_eng$vocabulary)
hebindiv_ling_eng$familiarity_std<-scale(hebindiv_ling_eng$familiarity)
m1 <- lm(accuracy~familiarity + vocabulary ,data=hebindiv_ling_eng)
summary(m1)
hebindiv_ling_eng$accuracy_std<-scale(hebindiv_ling_eng$accuracy)
hebindiv_ling_eng$vocabulary_std<-scale(hebindiv_ling_eng$vocabulary)
hebindiv_ling_eng$familiarity_std<-scale(hebindiv_ling_eng$familiarity)
m1 <- lm(accuracy_std~familiarity_std + vocabulary_std ,data=hebindiv_ling_eng)
summary(m1)
coef(m1)
hebindiv_ling_eng$accuracy_std<-scale(hebindiv_ling_eng$accuracy)
hebindiv_ling_eng$vocabulary_std<-scale(hebindiv_ling_eng$vocabulary)
hebindiv_ling_eng$familiarity_std<-scale(hebindiv_ling_eng$familiarity)
m1 <- lm(accuracy_std~familiarity_std + vocabulary_std ,data=hebindiv_ling_eng)
summary(m1)
coef(m1)
