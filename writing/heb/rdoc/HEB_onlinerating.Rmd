---
title: "HEB_Online_Rating"
author: "JM Schneider"
date: "12/8/2020"
output: html_document
---
### Import libraries
```{r loadlib, echo=T, results='hide', message=F, warning=F}
library(psych)
library(readr)
library(optimx)
library(corrplot)
library(reshape)
library(reshape2)
library(lmerTest)
library(ggplot2)
library(scales)
library(ggbeeswarm)
library(Hmisc)
library(arm)
library(ez)
library(dplyr)
library(arm)
library(tidyr)
library(tidyverse)
source("centerfactor.R")
```

### Load HEB Online Rating data output as dataframe

```{r}
setwd("/Volumes/data/projects/writing/heb/data_paper/data/")
df_eng=read.csv('../data/english.csv')
df_heb=read.csv('../data/hebrew.csv')
```

### Rename columns to words
```{r}
df_eng <- df_eng %>% rename(part_ID = Q4, gobuku = Q70_1, daropi = Q71_1, dabitu = Q68_1, bidapu = Q67_1, golatu = Q72_1, kubiro = Q73_1, pabiku = Q74_1, parodo = Q75_1, tibudo = Q76_1, tilapi = Q77_1, dukame = Q78_1, duloga = Q79_1, gedino = Q80_1, gekalu = Q81_1, kibeno = Q82_1, kimuga = Q83_1, nadime = Q84_1, nalobi = Q85_1, tobelu = Q86_1, tomubi = Q87_1)

df_heb <- df_heb %>% rename(part_ID = Q4, gobuku = Q70_1, daropi = Q71_1, dabitu = Q68_1, bidapu = Q67_1, golatu = Q72_1, kubiro = Q73_1, pabiku = Q74_1, parodo = Q75_1, tibudo = Q76_1, tilapi = Q77_1, dukame = Q78_1, duloga = Q79_1, gedino = Q80_1, gekalu = Q81_1, kibeno = Q82_1, kimuga = Q83_1, nadime = Q84_1, nalobi = Q85_1, tobelu = Q86_1, tomubi = Q87_1)
```


### Compute average rating on each English and Hebrew Item for both groups of Speakers
```{r}
df_eng = df_eng[-1,]
df_eng <- mutate_all(df_eng, function(x) as.numeric(as.character(x)))
colMeans(df_eng)

df_heb = df_heb[-1,]
df_heb <- mutate_all(df_heb, function(x) as.numeric(as.character(x)))
colMeans(df_heb)
```

### Compute average overall rating for English and Hebrew Items for both groups of Speakers
```{r}
df_eng$eng_item_mean <- rowMeans(subset(df_eng, select = c(3:12)), na.rm = TRUE)
df_eng$heb_item_mean <- rowMeans(subset(df_eng, select = c(13:22)), na.rm = TRUE)
df_heb$eng_item_mean <- rowMeans(subset(df_heb, select = c(3:12)), na.rm = TRUE)
df_heb$heb_item_mean <- rowMeans(subset(df_heb, select = c(13:22)), na.rm = TRUE)
```

## Statistics
### Omnibus ANOVA: Do English & Hebrew speakers significantly differ in their ratings of Hebrew and English words?
```{r}
df_heb = subset(df_heb, select = -c(Q27,Q28))
df_eng = subset(df_eng, select = -c(Q29))
df_heb$group <- rep("heb",nrow(df_heb))
df_eng$group <- rep("eng",nrow(df_eng))

df_omni<-rbind(df_eng,df_heb)
df_omni<-subset(df_omni, select = c(heb_item_mean,eng_item_mean,group))
df_omni$part_ID <- 1:100

library(reshape2)
df_omni <- melt(df_omni, id.vars=c("group","part_ID"),
    value.name = "item_mean")
library(rstatix)
res.aov <- anova_test(
  data = df_omni, dv = item_mean, wid = part_ID,
  between = group, within = variable
  )
get_anova_table(res.aov)
```

### Do English and Hebrew speakers significantly differ in their ratings of English-like words?
```{r}
t.test(df_eng$eng_item_mean,df_heb$eng_item_mean, alternative = "two.sided", var.equal = TRUE)

eng_eng_item_mean = mean(df_eng$eng_item_mean)
heb_eng_item_mean = mean(df_heb$eng_item_mean)
eng_eng_item_mean
heb_eng_item_mean
```
### Do English and Hebrew speakers significantly differ in their ratings of Hebrew-like words?
```{r}
t.test(df_eng$heb_item_mean,df_heb$heb_item_mean, alternative = "two.sided", var.equal = TRUE)

eng_heb_item_mean = mean(df_eng$heb_item_mean)
heb_heb_item_mean = mean(df_heb$heb_item_mean)
eng_heb_item_mean
heb_heb_item_mean
sd(df_eng$heb_item_mean)
sd(df_heb$heb_item_mean)
```
### Are the HEB foils rated the same or different than the HEB targets by English speakers?
```{r}
df_eng$target_mean <- rowMeans(subset(df_eng, select = c("dukame","gedino","kimuga","nalobi","tobelu")), na.rm = TRUE)
df_eng$foil_mean <- rowMeans(subset(df_eng, select = c("duloga", "nadime", "tomubi", "gekalu","kibeno")), na.rm = TRUE)

t.test(df_eng$target_mean,df_eng$foil_mean, paired = TRUE)
target_mean = mean(df_eng$target_mean)
foil_mean = mean(df_eng$foil_mean)
target_mean
foil_mean
```

### Are the HEB foils rated the same or different than the HEB targets by Hebrew speakers?
```{r}
df_heb$target_mean <- rowMeans(subset(df_heb, select = c("dukame","gedino","kimuga","nalobi","tobelu")), na.rm = TRUE)
df_heb$foil_mean <- rowMeans(subset(df_heb, select = c("duloga", "nadime", "tomubi", "gekalu","kibeno")), na.rm = TRUE)

t.test(df_heb$target_mean,df_heb$foil_mean, paired = TRUE)
heb_target_mean = mean(df_heb$target_mean)
heb_foil_mean = mean(df_heb$foil_mean)
heb_target_mean
heb_foil_mean
```

## Does the English speakersâ€™ rating of the HEB targets help explaining the learnability of these triplets? 
### Import original HEB Dataset
```{r}
hebtrial <- read.csv('../data/heb_trial_clean_082819.csv')
hebtrial$task = as.factor(as.character(hebtrial$task)) # 0 means nonlinguistic and 1 means linguistic task
hebtrial$task_order = as.factor(as.character(hebtrial$task_order)) # 0 means nonlinguistic task comes first and 1 means linguistic task comes first
hebtrial$trial_order = as.factor(as.character(hebtrial$trial_order)) # 0 means foil comes first and 1 means target comes first
hebtrial$subject = as.factor(as.character(hebtrial$subject)) 
hebtrial$trial = as.factor(as.character(hebtrial$trial)) 
heb_only_trial<- hebtrial[which(hebtrial$language == "hebrew"),]
eng_trial<- hebtrial[which(hebtrial$language == "english"),]
eng_ling_trial<- eng_trial[which(eng_trial$task == 1),]
eng_trial=droplevels(eng_trial)
summary(eng_trial)
```

### Add web based familiarity ratings with larger original HEB dataset
```{r}
eng_ling_trial$web_ratings <- ifelse(eng_ling_trial$trial_number == 1 | eng_ling_trial$trial_number == 2 | eng_ling_trial$trial_number == 3 | eng_ling_trial$trial_number == 4 | eng_ling_trial$trial_number == 5, "2.18", #DUKAME
                          ifelse(eng_ling_trial$trial_number == 6 | eng_ling_trial$trial_number == 7 | eng_ling_trial$trial_number == 8 | eng_ling_trial$trial_number == 9 | eng_ling_trial$trial_number == 10, "2.40", #GEDINO
                                 ifelse(eng_ling_trial$trial_number == 11 | eng_ling_trial$trial_number == 12 | eng_ling_trial$trial_number == 13 | eng_ling_trial$trial_number == 14 | eng_ling_trial$trial_number == 15, "1.90", #KIMUGA
                                        ifelse(eng_ling_trial$trial_number == 16 | eng_ling_trial$trial_number == 17 | eng_ling_trial$trial_number == 18 | eng_ling_trial$trial_number == 19 | eng_ling_trial$trial_number == 20, "2.50", #NALOBI
                                               ifelse(eng_ling_trial$trial_number == 21 | eng_ling_trial$trial_number == 22 | eng_ling_trial$trial_number == 23 | eng_ling_trial$trial_number == 24 | eng_ling_trial$trial_number == 25, "2.20","0"))))) #TOBELU
```


```{r}
eng_ling_trial$triplet <- ifelse(eng_ling_trial$trial_number == 1 | eng_ling_trial$trial_number == 2 | eng_ling_trial$trial_number == 3 | eng_ling_trial$trial_number == 4 | eng_ling_trial$trial_number == 5, "2", #DUKAME
                          ifelse(eng_ling_trial$trial_number == 6 | eng_ling_trial$trial_number == 7 | eng_ling_trial$trial_number == 8 | eng_ling_trial$trial_number == 9 | eng_ling_trial$trial_number == 10, "1", #GEDINO
                                 ifelse(eng_ling_trial$trial_number == 11 | eng_ling_trial$trial_number == 12 | eng_ling_trial$trial_number == 13 | eng_ling_trial$trial_number == 14 | eng_ling_trial$trial_number == 15, "3", #KIMUGA
                                        ifelse(eng_ling_trial$trial_number == 16 | eng_ling_trial$trial_number == 17 | eng_ling_trial$trial_number == 18 | eng_ling_trial$trial_number == 19 | eng_ling_trial$trial_number == 20, "4", #NALOBI
                                               ifelse(eng_ling_trial$trial_number == 21 | eng_ling_trial$trial_number == 22 | eng_ling_trial$trial_number == 23 | eng_ling_trial$trial_number == 24 | eng_ling_trial$trial_number == 25, "5","0"))))) #TOBELU
```

### A generalized linear mixed-effects analysis with trial-by-trial perceived familiarity (using ratings from the web-based sample) as the fixed effect, as well as random slopes for subject and question number.
```{r}
eng_ling_trial$web_ratings=as.numeric(as.character(eng_ling_trial$web_ratings))
fm2 <-  glmer(trial_accuracy~web_ratings+(1+web_ratings|subject)+(1+web_ratings|triplet),family = binomial,data=eng_ling_trial)
summary(fm2)
```
## Does accuracy significantly differ across ratings? No.
```{r}
m1 = lm(trial_accuracy~web_ratings,data=df)
summary(m1)

m1 = lm(trial_accuracy~fam_ratings,data=df)
summary(m1)
```


## Are the target words rated differently in English-speaking population?
```{r}
library(dunn.test)
#df_merge<-rbind(df_eng,df_heb)
webrank=df_eng[c(1,12, 14, 17, 19,20)]
webrank$part_ID<-1:50

webrank=melt(webrank,id=1)
colnames(webrank)[2]="item"
colnames(webrank)[3]="ranking"
kruskal.test(ranking ~ item,
             data = webrank)
DT = dunn.test(webrank$ranking,webrank$item, method="bh")      # Adjusts p-values for multiple comparisons;
```

## Are the target words rated differently before and after training? The two groups have different ranking baseline and ranking procedure (therefore not comparable)